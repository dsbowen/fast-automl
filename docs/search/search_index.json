{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fast AutoML Most autoML packages aim for exceptional performance but need to train for an exceptional amount of time. Fast-autoML aims for reasonable performance in a reasonable amount of time. Fast-autoML includes additional utilities, such as tools for comparing model performance by repeated cross-validation. Installation $ pip install fast-automl Quickstart from fast_automl.automl import AutoClassifier from sklearn.datasets import load_digits from sklearn.model_selection import cross_val_score, train_test_split X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y) clf = AutoClassifier(ensemble_method='stepwise', n_jobs=-1, verbose=True).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test))) This runs for about 6-7 minutes and typically achieves a test accuracy of 96-99% and ROC AUC above .999. from fast_automl.automl import AutoRegressor from sklearn.datasets import load_diabetes from sklearn.model_selection import cross_val_score, train_test_split X, y = load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = AutoRegressor(n_jobs=-1, verbose=True).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(reg.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(reg.score(X_test, y_test))) This runs for about 30 seconds and typically achieves a test R-squared of .47-.53. Citation @software{bowen2021fast-automl, author = {Dillon Bowen}, title = {Fast-AutoML}, url = {https://dsbowen.github.io/fast-automl/}, date = {2021-02-05}, } License Users must cite this package in any publications which use it. It is licensed with the MIT License . Acknowledgments This package and its documentation draw heavily on scikit-learn .","title":"Home"},{"location":"#fast-automl","text":"Most autoML packages aim for exceptional performance but need to train for an exceptional amount of time. Fast-autoML aims for reasonable performance in a reasonable amount of time. Fast-autoML includes additional utilities, such as tools for comparing model performance by repeated cross-validation.","title":"Fast AutoML"},{"location":"#installation","text":"$ pip install fast-automl","title":"Installation"},{"location":"#quickstart","text":"from fast_automl.automl import AutoClassifier from sklearn.datasets import load_digits from sklearn.model_selection import cross_val_score, train_test_split X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y) clf = AutoClassifier(ensemble_method='stepwise', n_jobs=-1, verbose=True).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test))) This runs for about 6-7 minutes and typically achieves a test accuracy of 96-99% and ROC AUC above .999. from fast_automl.automl import AutoRegressor from sklearn.datasets import load_diabetes from sklearn.model_selection import cross_val_score, train_test_split X, y = load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = AutoRegressor(n_jobs=-1, verbose=True).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(reg.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(reg.score(X_test, y_test))) This runs for about 30 seconds and typically achieves a test R-squared of .47-.53.","title":"Quickstart"},{"location":"#citation","text":"@software{bowen2021fast-automl, author = {Dillon Bowen}, title = {Fast-AutoML}, url = {https://dsbowen.github.io/fast-automl/}, date = {2021-02-05}, }","title":"Citation"},{"location":"#license","text":"Users must cite this package in any publications which use it. It is licensed with the MIT License .","title":"License"},{"location":"#acknowledgments","text":"This package and its documentation draw heavily on scikit-learn .","title":"Acknowledgments"},{"location":"contribute/","text":"Contribute I welcome contributions to this project, including: Bayesian optimiziation tools for autoML Additional CV estimators Other practical ML utilities not found or not well implemented in other ML packages.","title":"Contribute"},{"location":"contribute/#contribute","text":"I welcome contributions to this project, including: Bayesian optimiziation tools for autoML Additional CV estimators Other practical ML utilities not found or not well implemented in other ML packages.","title":"Contribute"},{"location":"api/automl/","text":"a.src-href { float: right; } p.attr { margin-top: 0.5em; margin-left: 1em; } p.func-header { background-color: gainsboro; border-radius: 0.1em; padding: 0.5em; padding-left: 1em; } table.field-table { border-radius: 0.1em } Automated machine learning fast_automl.automl. make_cv_regressors def fast_automl.automl. make_cv_regressors ( ) [source] Returns: cv_regressors : list List of default CV regresssors. fast_automl.automl. make_cv_classifiers def fast_automl.automl. make_cv_classifiers ( ) [source] Returns: cv_classifiers : list List of default CV classifiers. fast_automl.automl. AutoEstimator class fast_automl.automl. AutoEstimator ( cv_estimators=[], preprocessors=[], ensemble_method= 'auto', max_ensemble_size=50, n_ensembles=1, n_iter=10, n_jobs=None, verbose=False, cv=None, scoring=None ) [source] Parameters: cv_estimators : list of CVEstimators, default=[] If an empty list, a default list of CVEstimators will be created. preprocessors : list, default=[] List of preprocessing steps before data is fed to the cv_estimators . ensemble_method : str, default='auto' If 'rfe' , the ensemble is created using recursive feature elimination. If 'stepwise' , the ensemble is created using stepwise addition. If 'auto' , the ensemble is the better of the RFE and stepwise ensemble methods. max_ensemble_size : int, default=50 The maximum number of estimators to consider adding to the ensemble. n_ensembles : int, default=1 Number of ensembles to create using different CV splits. These ensembles get equal votes in a meta-ensemble. n_iter : int, default=10 Number of iterations to run randomized search for the CVEstimators. n_jobs : int or None, default=None Number of jobs to run in parallel. verbose : bool, default=False Controls the verbosity. cv : int, cross-validation generator, or iterable, default=None Scikit-learn style cv parameter. scoring : str, callable, list, tuple, or dict, default=None Scikit-learn style scoring parameter. By default, a regressor ensembles maximizes R-squared and a classifier ensemble maximizes ROC AUC. Attributes: best_estimator_ : estimator Ensemble or meta-ensemble associated with the best CV score. Methods fit ( self, X, y, sample_weight=None ) [source] Fit the model. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Predict class labels for samples in X. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: C : array of shape (n_samples,) Predicted class label for each sample. predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes on the model, ordered by self.classes_ . fast_automl.automl. AutoClassifier Automatic classifier. Inherits from AutoEstimator . Examples from fast_automl.automl import AutoClassifier from sklearn.datasets import load_digits from sklearn.model_selection import cross_val_score, train_test_split X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y) clf = AutoClassifier(ensemble_method='stepwise', n_jobs=-1, verbose=True).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test))) This runs for about 6-7 minutes and typically achieves a test accuracy of 96-99% and ROC AUC above .999. fast_automl.automl. AutoRegressor Automatic regressor. Inherits from AutoEstimator . Examples from fast_automl.automl import AutoRegressor from sklearn.datasets import load_diabetes from sklearn.model_selection import cross_val_score, train_test_split X, y = load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = AutoRegressor(n_jobs=-1, verbose=True).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(reg.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(reg.score(X_test, y_test))) This runs for about 30 seconds and typically achieves a test R-squared of .47-.53.","title":"Auto ML"},{"location":"api/automl/#automated-machine-learning","text":"","title":"Automated machine learning"},{"location":"api/automl/#fast_automlautomlmake_cv_regressors","text":"def fast_automl.automl. make_cv_regressors ( ) [source] Returns: cv_regressors : list List of default CV regresssors.","title":"fast_automl.automl.make_cv_regressors"},{"location":"api/automl/#fast_automlautomlmake_cv_classifiers","text":"def fast_automl.automl. make_cv_classifiers ( ) [source] Returns: cv_classifiers : list List of default CV classifiers.","title":"fast_automl.automl.make_cv_classifiers"},{"location":"api/automl/#fast_automlautomlautoestimator","text":"class fast_automl.automl. AutoEstimator ( cv_estimators=[], preprocessors=[], ensemble_method= 'auto', max_ensemble_size=50, n_ensembles=1, n_iter=10, n_jobs=None, verbose=False, cv=None, scoring=None ) [source] Parameters: cv_estimators : list of CVEstimators, default=[] If an empty list, a default list of CVEstimators will be created. preprocessors : list, default=[] List of preprocessing steps before data is fed to the cv_estimators . ensemble_method : str, default='auto' If 'rfe' , the ensemble is created using recursive feature elimination. If 'stepwise' , the ensemble is created using stepwise addition. If 'auto' , the ensemble is the better of the RFE and stepwise ensemble methods. max_ensemble_size : int, default=50 The maximum number of estimators to consider adding to the ensemble. n_ensembles : int, default=1 Number of ensembles to create using different CV splits. These ensembles get equal votes in a meta-ensemble. n_iter : int, default=10 Number of iterations to run randomized search for the CVEstimators. n_jobs : int or None, default=None Number of jobs to run in parallel. verbose : bool, default=False Controls the verbosity. cv : int, cross-validation generator, or iterable, default=None Scikit-learn style cv parameter. scoring : str, callable, list, tuple, or dict, default=None Scikit-learn style scoring parameter. By default, a regressor ensembles maximizes R-squared and a classifier ensemble maximizes ROC AUC. Attributes: best_estimator_ : estimator Ensemble or meta-ensemble associated with the best CV score.","title":"fast_automl.automl.AutoEstimator"},{"location":"api/automl/#methods","text":"fit ( self, X, y, sample_weight=None ) [source] Fit the model. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Predict class labels for samples in X. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: C : array of shape (n_samples,) Predicted class label for each sample. predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes on the model, ordered by self.classes_ .","title":"Methods"},{"location":"api/automl/#fast_automlautomlautoclassifier","text":"Automatic classifier. Inherits from AutoEstimator .","title":"fast_automl.automl.AutoClassifier"},{"location":"api/automl/#examples","text":"from fast_automl.automl import AutoClassifier from sklearn.datasets import load_digits from sklearn.model_selection import cross_val_score, train_test_split X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y) clf = AutoClassifier(ensemble_method='stepwise', n_jobs=-1, verbose=True).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test))) This runs for about 6-7 minutes and typically achieves a test accuracy of 96-99% and ROC AUC above .999.","title":"Examples"},{"location":"api/automl/#fast_automlautomlautoregressor","text":"Automatic regressor. Inherits from AutoEstimator .","title":"fast_automl.automl.AutoRegressor"},{"location":"api/automl/#examples_1","text":"from fast_automl.automl import AutoRegressor from sklearn.datasets import load_diabetes from sklearn.model_selection import cross_val_score, train_test_split X, y = load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = AutoRegressor(n_jobs=-1, verbose=True).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(reg.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(reg.score(X_test, y_test))) This runs for about 30 seconds and typically achieves a test R-squared of .47-.53.","title":"Examples"},{"location":"api/baseline/","text":"a.src-href { float: right; } p.attr { margin-top: 0.5em; margin-left: 1em; } p.func-header { background-color: gainsboro; border-radius: 0.1em; padding: 0.5em; padding-left: 1em; } table.field-table { border-radius: 0.1em } Baseline classifier and regressor fast_automl.baseline. BaselineClassifier Predicts the most frequent class. Attributes: classes_ : array-like of shape (n_classes,) A list of class weights known to the classifier. counts_ : array-like of shape (n_classes,) Normalized frequency of each class in the training data. dominant_class_ : int Class which appears most frequently in the training data. Examples from fast_automl.baseline import BaselineClassifier from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) clf = BaselineClassifier().fit(X_train, y_train) clf.score(X_test, y_test) Methods fit ( self, X, y, sample_weight=None ) [source] Fit the model. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Predict class labels for samples in X. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: C : array of shape (n_samples,) Predicted class label for each sample. predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes on the model, ordered by self.classes_ . fast_automl.baseline. BaselineRegressor Predicts the mean target value. Attributes: y_mean_ : np.array Average target value. Examples from fast_automl.baseline import BaselineRegressor from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split X, y = load_boston(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y) reg = BaselineRegressor().fit(X_train, y_train) reg.score(X_test, y_test) Methods fit ( self, X, y, sample_weight=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples, n_targets) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Parameters: X : array-like, shape (n_samples, n_features) Samples. Returns: C : array, shape (n_samples, n_targets) Predicted values","title":"Baseline estimators"},{"location":"api/baseline/#baseline-classifier-and-regressor","text":"","title":"Baseline classifier and regressor"},{"location":"api/baseline/#fast_automlbaselinebaselineclassifier","text":"Predicts the most frequent class. Attributes: classes_ : array-like of shape (n_classes,) A list of class weights known to the classifier. counts_ : array-like of shape (n_classes,) Normalized frequency of each class in the training data. dominant_class_ : int Class which appears most frequently in the training data.","title":"fast_automl.baseline.BaselineClassifier"},{"location":"api/baseline/#examples","text":"from fast_automl.baseline import BaselineClassifier from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) clf = BaselineClassifier().fit(X_train, y_train) clf.score(X_test, y_test)","title":"Examples"},{"location":"api/baseline/#methods","text":"fit ( self, X, y, sample_weight=None ) [source] Fit the model. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Predict class labels for samples in X. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: C : array of shape (n_samples,) Predicted class label for each sample. predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes on the model, ordered by self.classes_ .","title":"Methods"},{"location":"api/baseline/#fast_automlbaselinebaselineregressor","text":"Predicts the mean target value. Attributes: y_mean_ : np.array Average target value.","title":"fast_automl.baseline.BaselineRegressor"},{"location":"api/baseline/#examples_1","text":"from fast_automl.baseline import BaselineRegressor from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split X, y = load_boston(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y) reg = BaselineRegressor().fit(X_train, y_train) reg.score(X_test, y_test)","title":"Examples"},{"location":"api/baseline/#methods_1","text":"fit ( self, X, y, sample_weight=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples, n_targets) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Parameters: X : array-like, shape (n_samples, n_features) Samples. Returns: C : array, shape (n_samples, n_targets) Predicted values","title":"Methods"},{"location":"api/cv_estimators/","text":"a.src-href { float: right; } p.attr { margin-top: 0.5em; margin-left: 1em; } p.func-header { background-color: gainsboro; border-radius: 0.1em; padding: 0.5em; padding-left: 1em; } table.field-table { border-radius: 0.1em } Cross-validation estimators Examples from fast_automl.cv_estimators import RandomForestClassifierCV from sklearn.datasets import load_digits from sklearn.model_selection import cross_val_score, train_test_split X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True) clf = RandomForestClassifierCV().fit(X_train, y_train, n_jobs=-1) print('Cross val score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test))) Out: Cross val score: 0.9696 Test score: 0.9800 fast_automl.cv_estimators. CVBaseEstimator class fast_automl.cv_estimators. CVBaseEstimator ( preprocessors=[], param_distributions={} ) [source] Base class for all CV estimators. Parameters: preprocessors : list, default=[] Preprocessing steps. param_distributions : dict, default={} Maps names of parameters to distributions. This overrides parameters returned by the get_param_distributions method. Attributes: best_estimator_ : estimator Estimator which attained the best CV score under randomized search. best_score_ : scalar Best CV score attained by any estimator. cv_results_ : list List of (mean CV score, parameters) tuples. Methods get_param_distributions ( self, param_distributions={} ) [source] Parameters: param_distributions : dict, default={} These are overridden by the param_distributions parameter passed to the constructor. Returns: param_distributions : dict Parameter distributions used for randomized search. make_estimator ( self, **params ) [source] fit ( category=ConvergenceWarning) def fit(self, X, y, n_iter=10, n_jobs=None, scoring=None ) [source] Fits a CV estimator. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. n_iter : int, default=10 Number of iterations to use in randomized search. n_jobs : int or None, default=None Number of background jobs to use in randomized search. scoring : str or callable, default=None A str (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y) which should return only a single value. If None , the estimator's default score method is used. predict ( self, X ) [source] Parameters: X : array-like, shape (n_samples, n_features) Samples. Returns: C : array, shape (n_samples, n_targets) Predicted values predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes on the model, ordered by self.classes_ . Notes Only applicable for classifiers. fast_automl.cv_estimators. RandomForestClassifierCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCARandomForestClassifierCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. RandomForestRegressorCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCARandomForestRegressorCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. LogisticLassoCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCALogisticLassoCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. LassoLarsCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCALassoLarsCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. LogisticRidgeCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCALogisticRidgeCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. RidgeCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCARidgeCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. LogisticElasticNetCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCALogisticElasticNetCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. ElasticNetCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCAElasticNetCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. KernelRidgeCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCAKernelRidgeCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. SVCCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCASVCCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. SVRCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCASVRCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. KNeighborsClassifierCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCAKNeighborsClassifierCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. KNeighborsRegressorCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCAKNeighborsRegressorCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. AdaBoostClassifierCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCAAdaBoostClassifierCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. AdaBoostRegressorCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCAAdaBoostRegressorCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. XGBClassifierCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCAXGBClassifierCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. XGBRegressorCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source] fast_automl.cv_estimators. PCAXGBRegressorCV Methods make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"CV estimators"},{"location":"api/cv_estimators/#cross-validation-estimators","text":"","title":"Cross-validation estimators"},{"location":"api/cv_estimators/#examples","text":"from fast_automl.cv_estimators import RandomForestClassifierCV from sklearn.datasets import load_digits from sklearn.model_selection import cross_val_score, train_test_split X, y = load_digits(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True) clf = RandomForestClassifierCV().fit(X_train, y_train, n_jobs=-1) print('Cross val score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test))) Out: Cross val score: 0.9696 Test score: 0.9800","title":"Examples"},{"location":"api/cv_estimators/#fast_automlcv_estimatorscvbaseestimator","text":"class fast_automl.cv_estimators. CVBaseEstimator ( preprocessors=[], param_distributions={} ) [source] Base class for all CV estimators. Parameters: preprocessors : list, default=[] Preprocessing steps. param_distributions : dict, default={} Maps names of parameters to distributions. This overrides parameters returned by the get_param_distributions method. Attributes: best_estimator_ : estimator Estimator which attained the best CV score under randomized search. best_score_ : scalar Best CV score attained by any estimator. cv_results_ : list List of (mean CV score, parameters) tuples.","title":"fast_automl.cv_estimators.CVBaseEstimator"},{"location":"api/cv_estimators/#methods","text":"get_param_distributions ( self, param_distributions={} ) [source] Parameters: param_distributions : dict, default={} These are overridden by the param_distributions parameter passed to the constructor. Returns: param_distributions : dict Parameter distributions used for randomized search. make_estimator ( self, **params ) [source] fit ( category=ConvergenceWarning) def fit(self, X, y, n_iter=10, n_jobs=None, scoring=None ) [source] Fits a CV estimator. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. n_iter : int, default=10 Number of iterations to use in randomized search. n_jobs : int or None, default=None Number of background jobs to use in randomized search. scoring : str or callable, default=None A str (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y) which should return only a single value. If None , the estimator's default score method is used. predict ( self, X ) [source] Parameters: X : array-like, shape (n_samples, n_features) Samples. Returns: C : array, shape (n_samples, n_targets) Predicted values predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes on the model, ordered by self.classes_ .","title":"Methods"},{"location":"api/cv_estimators/#notes","text":"Only applicable for classifiers.","title":"Notes"},{"location":"api/cv_estimators/#fast_automlcv_estimatorsrandomforestclassifiercv","text":"","title":"fast_automl.cv_estimators.RandomForestClassifierCV"},{"location":"api/cv_estimators/#methods_1","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcarandomforestclassifiercv","text":"","title":"fast_automl.cv_estimators.PCARandomForestClassifierCV"},{"location":"api/cv_estimators/#methods_2","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorsrandomforestregressorcv","text":"","title":"fast_automl.cv_estimators.RandomForestRegressorCV"},{"location":"api/cv_estimators/#methods_3","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcarandomforestregressorcv","text":"","title":"fast_automl.cv_estimators.PCARandomForestRegressorCV"},{"location":"api/cv_estimators/#methods_4","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorslogisticlassocv","text":"","title":"fast_automl.cv_estimators.LogisticLassoCV"},{"location":"api/cv_estimators/#methods_5","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcalogisticlassocv","text":"","title":"fast_automl.cv_estimators.PCALogisticLassoCV"},{"location":"api/cv_estimators/#methods_6","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorslassolarscv","text":"","title":"fast_automl.cv_estimators.LassoLarsCV"},{"location":"api/cv_estimators/#methods_7","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcalassolarscv","text":"","title":"fast_automl.cv_estimators.PCALassoLarsCV"},{"location":"api/cv_estimators/#methods_8","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorslogisticridgecv","text":"","title":"fast_automl.cv_estimators.LogisticRidgeCV"},{"location":"api/cv_estimators/#methods_9","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcalogisticridgecv","text":"","title":"fast_automl.cv_estimators.PCALogisticRidgeCV"},{"location":"api/cv_estimators/#methods_10","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorsridgecv","text":"","title":"fast_automl.cv_estimators.RidgeCV"},{"location":"api/cv_estimators/#methods_11","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcaridgecv","text":"","title":"fast_automl.cv_estimators.PCARidgeCV"},{"location":"api/cv_estimators/#methods_12","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorslogisticelasticnetcv","text":"","title":"fast_automl.cv_estimators.LogisticElasticNetCV"},{"location":"api/cv_estimators/#methods_13","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcalogisticelasticnetcv","text":"","title":"fast_automl.cv_estimators.PCALogisticElasticNetCV"},{"location":"api/cv_estimators/#methods_14","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorselasticnetcv","text":"","title":"fast_automl.cv_estimators.ElasticNetCV"},{"location":"api/cv_estimators/#methods_15","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcaelasticnetcv","text":"","title":"fast_automl.cv_estimators.PCAElasticNetCV"},{"location":"api/cv_estimators/#methods_16","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorskernelridgecv","text":"","title":"fast_automl.cv_estimators.KernelRidgeCV"},{"location":"api/cv_estimators/#methods_17","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcakernelridgecv","text":"","title":"fast_automl.cv_estimators.PCAKernelRidgeCV"},{"location":"api/cv_estimators/#methods_18","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorssvccv","text":"","title":"fast_automl.cv_estimators.SVCCV"},{"location":"api/cv_estimators/#methods_19","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcasvccv","text":"","title":"fast_automl.cv_estimators.PCASVCCV"},{"location":"api/cv_estimators/#methods_20","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorssvrcv","text":"","title":"fast_automl.cv_estimators.SVRCV"},{"location":"api/cv_estimators/#methods_21","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcasvrcv","text":"","title":"fast_automl.cv_estimators.PCASVRCV"},{"location":"api/cv_estimators/#methods_22","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorskneighborsclassifiercv","text":"","title":"fast_automl.cv_estimators.KNeighborsClassifierCV"},{"location":"api/cv_estimators/#methods_23","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcakneighborsclassifiercv","text":"","title":"fast_automl.cv_estimators.PCAKNeighborsClassifierCV"},{"location":"api/cv_estimators/#methods_24","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorskneighborsregressorcv","text":"","title":"fast_automl.cv_estimators.KNeighborsRegressorCV"},{"location":"api/cv_estimators/#methods_25","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcakneighborsregressorcv","text":"","title":"fast_automl.cv_estimators.PCAKNeighborsRegressorCV"},{"location":"api/cv_estimators/#methods_26","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorsadaboostclassifiercv","text":"","title":"fast_automl.cv_estimators.AdaBoostClassifierCV"},{"location":"api/cv_estimators/#methods_27","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcaadaboostclassifiercv","text":"","title":"fast_automl.cv_estimators.PCAAdaBoostClassifierCV"},{"location":"api/cv_estimators/#methods_28","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorsadaboostregressorcv","text":"","title":"fast_automl.cv_estimators.AdaBoostRegressorCV"},{"location":"api/cv_estimators/#methods_29","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcaadaboostregressorcv","text":"","title":"fast_automl.cv_estimators.PCAAdaBoostRegressorCV"},{"location":"api/cv_estimators/#methods_30","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorsxgbclassifiercv","text":"","title":"fast_automl.cv_estimators.XGBClassifierCV"},{"location":"api/cv_estimators/#methods_31","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcaxgbclassifiercv","text":"","title":"fast_automl.cv_estimators.PCAXGBClassifierCV"},{"location":"api/cv_estimators/#methods_32","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorsxgbregressorcv","text":"","title":"fast_automl.cv_estimators.XGBRegressorCV"},{"location":"api/cv_estimators/#methods_33","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/cv_estimators/#fast_automlcv_estimatorspcaxgbregressorcv","text":"","title":"fast_automl.cv_estimators.PCAXGBRegressorCV"},{"location":"api/cv_estimators/#methods_34","text":"make_estimator ( self, **params ) [source] get_param_distributions ( self, X, y ) [source]","title":"Methods"},{"location":"api/ensemble/","text":"a.src-href { float: right; } p.attr { margin-top: 0.5em; margin-left: 1em; } p.func-header { background-color: gainsboro; border-radius: 0.1em; padding: 0.5em; padding-left: 1em; } table.field-table { border-radius: 0.1em } Ensemble fast_automl.ensemble. ClassifierWeighter class fast_automl.ensemble. ClassifierWeighter ( loss=log_loss ) [source] Trains weights for ensemble of classifiers. Parameters: loss : callable, default=log_loss Loss function to minimize by weight fitting. Attributes: coef_ : array-like of shape (n_estimators,) Weights on the given estimators. Examples from fast_automl.ensemble import ClassifierWeighter import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import StratifiedKFold, cross_val_predict, train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from copy import deepcopy X, y = load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True) svc = SVC(probability=True).fit(X_train, y_train) knn = KNeighborsClassifier().fit(X_train, y_train) cv = StratifiedKFold(random_state=np.random.RandomState(), shuffle=True) X_meta = np.array([ cross_val_predict(clf, X_train, y_train, cv=deepcopy(cv), method='predict_proba') for clf in (svc, knn) ]).transpose(1, 2, 0) weighter = ClassifierWeighter().fit(X_meta, y_train) X_meta_test = np.array([ clf.predict_proba(X_test) for clf in (svc, knn) ]).transpose(1, 2, 0) weighter.score(X_meta_test, y_test) Methods fit ( self, X, y ) [source] Parameters: X : array-like of shape (n_samples, n_classes, n_estimators) X_ice is the probability estimator e puts on sample i being in class c. y : array-like of shape (n_samples,) Targets. Returns: self : predict ( self, X ) [source] Predict class labels for samples in X. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: C : array of shape (n_samples,) Predicted class label for each sample. predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes. fast_automl.ensemble. BaseStackingCV class fast_automl.ensemble. BaseStackingCV ( estimators, cv=None, shuffle_cv=True, scoring=None, n_jobs=None, verbose=0 ) [source] Parameters: estimators : list Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. cv : int, cross-validation generator, or iterable, default=None Scikit-learn style cv parameter. shuffle_cv : bool, default=True Indicates that cross validator should shuffle observations. scoring : str, callable, list, tuple, or dict, default=None Scikit-learn style scoring parameter. n_jobs : int, default=None Number of jobs to run in parallel. verbose : int, default=0 Controls the verbosity. Methods fit ( self, X, y, sample_weight=None ) [source] predict ( self, X ) [source] Predict class labels for samples in X. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: C : array of shape (n_samples,) Predicted outcome for each sample. predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes on the model. Notes Only applicable to classifiers. transform ( self, X ) [source] Transforms raw features into a prediction matrix. Parameters: X : array-like of shape (n_samples, n_features) Features. Returns: X_meta : array-like Prediction matrix of shape (n_samples, n_estimators) for regression and (n_estimators, n_samples, n_classes) for classification. fast_automl.ensemble. RFEVotingEstimatorCV Selects estimators using recursive feature elimination. Inherits from BaseStackingCV . Attributes: best_estimator_ : estimator The voting estimator associated with the highest CV score. best_score_ : scalar Highest CV score attained by any voting estimator. weights_ : array-like Weights the voting estimator places on each of the estimators in its ensemble. names_ : list List of estimator names in the best estimator. Methods fit ( self, X, y, sample_weight=None ) [source] Fit the model. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : fast_automl.ensemble. RFEVotingClassifierCV Selects classifiers using recursive feature elimination. Inherits from RFEVotingEstimatorCV . Examples from fast_automl.ensemble import RFEVotingClassifierCV from sklearn.datasets import load_breast_cancer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score, train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC X, y = load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True) clf = RFEVotingClassifierCV([ ('rf', RandomForestClassifier()), ('knn', KNeighborsClassifier()), ('svm', SVC(probability=True)) ]).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test))) fast_automl.ensemble. RFEVotingRegressorCV Selects regressors using recursive feature elimination. Inherits from RFEVotingEstimatorCV . Examples from fast_automl.ensemble import RFEVotingRegressorCV from sklearn.datasets import load_boston from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score, train_test_split from sklearn.neighbors import KNeighborsRegressor from sklearn.svm import SVR X, y = load_boston(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = RFEVotingRegressorCV([ ('rf', RandomForestRegressor()), ('knn', KNeighborsRegressor()), ('svm', SVR()) ]).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(reg.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(reg.score(X_test, y_test))) fast_automl.ensemble. StepwiseVotingEstimatorCV Selects estimators using stepwise addition. Inherits from BaseStackingCV . Attributes: best_estimator_ : estimator The voting estimator associated with the highest CV score. best_score_ : scalar Highest CV score attained by any voting estimator. weights_ : array-like Weights the voting estimator places on each of the estimators in its ensemble. names_ : list List of estimator names in the best estimator. Methods fit ( self, X, y, sample_weight=None ) [source] Fit the model. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : fast_automl.ensemble. StepwiseVotingClassifierCV Selects classifiers using stepwise addition. Inherits from StepwiseVotingEstimatorCV . Examples from fast_automl.ensemble import StepwiseVotingClassifierCV from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score, train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC X, y = load_iris(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True) clf = StepwiseVotingClassifierCV([ ('rf', RandomForestClassifier()), ('knn', KNeighborsClassifier()), ('svm', SVC(probability=True)) ]).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test))) fast_automl.ensemble. StepwiseVotingRegressorCV Selects regressors using stepwise addition. Inherits from StepwiseVotingEstimatorCV . Examples from fast_automl.ensemble import StepwiseVotingRegressorCV from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score, train_test_split from sklearn.neighbors import KNeighborsRegressor from sklearn.svm import SVR X, y = load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = StepwiseVotingRegressorCV([ ('rf', RandomForestRegressor()), ('knn', KNeighborsRegressor()), ('svm', SVR()) ]).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(reg.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(reg.score(X_test, y_test)))","title":"Ensemble"},{"location":"api/ensemble/#ensemble","text":"","title":"Ensemble"},{"location":"api/ensemble/#fast_automlensembleclassifierweighter","text":"class fast_automl.ensemble. ClassifierWeighter ( loss=log_loss ) [source] Trains weights for ensemble of classifiers. Parameters: loss : callable, default=log_loss Loss function to minimize by weight fitting. Attributes: coef_ : array-like of shape (n_estimators,) Weights on the given estimators.","title":"fast_automl.ensemble.ClassifierWeighter"},{"location":"api/ensemble/#examples","text":"from fast_automl.ensemble import ClassifierWeighter import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import StratifiedKFold, cross_val_predict, train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from copy import deepcopy X, y = load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True) svc = SVC(probability=True).fit(X_train, y_train) knn = KNeighborsClassifier().fit(X_train, y_train) cv = StratifiedKFold(random_state=np.random.RandomState(), shuffle=True) X_meta = np.array([ cross_val_predict(clf, X_train, y_train, cv=deepcopy(cv), method='predict_proba') for clf in (svc, knn) ]).transpose(1, 2, 0) weighter = ClassifierWeighter().fit(X_meta, y_train) X_meta_test = np.array([ clf.predict_proba(X_test) for clf in (svc, knn) ]).transpose(1, 2, 0) weighter.score(X_meta_test, y_test)","title":"Examples"},{"location":"api/ensemble/#methods","text":"fit ( self, X, y ) [source] Parameters: X : array-like of shape (n_samples, n_classes, n_estimators) X_ice is the probability estimator e puts on sample i being in class c. y : array-like of shape (n_samples,) Targets. Returns: self : predict ( self, X ) [source] Predict class labels for samples in X. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: C : array of shape (n_samples,) Predicted class label for each sample. predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes.","title":"Methods"},{"location":"api/ensemble/#fast_automlensemblebasestackingcv","text":"class fast_automl.ensemble. BaseStackingCV ( estimators, cv=None, shuffle_cv=True, scoring=None, n_jobs=None, verbose=0 ) [source] Parameters: estimators : list Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. cv : int, cross-validation generator, or iterable, default=None Scikit-learn style cv parameter. shuffle_cv : bool, default=True Indicates that cross validator should shuffle observations. scoring : str, callable, list, tuple, or dict, default=None Scikit-learn style scoring parameter. n_jobs : int, default=None Number of jobs to run in parallel. verbose : int, default=0 Controls the verbosity.","title":"fast_automl.ensemble.BaseStackingCV"},{"location":"api/ensemble/#methods_1","text":"fit ( self, X, y, sample_weight=None ) [source] predict ( self, X ) [source] Predict class labels for samples in X. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: C : array of shape (n_samples,) Predicted outcome for each sample. predict_proba ( self, X ) [source] Probability estimates. Parameters: X : array-like of shape (n_samples, n_features) Samples. Returns: T : array-like of shape (n_samples, n_classes) Probability of the sample for each classes on the model.","title":"Methods"},{"location":"api/ensemble/#notes","text":"Only applicable to classifiers. transform ( self, X ) [source] Transforms raw features into a prediction matrix. Parameters: X : array-like of shape (n_samples, n_features) Features. Returns: X_meta : array-like Prediction matrix of shape (n_samples, n_estimators) for regression and (n_estimators, n_samples, n_classes) for classification.","title":"Notes"},{"location":"api/ensemble/#fast_automlensemblerfevotingestimatorcv","text":"Selects estimators using recursive feature elimination. Inherits from BaseStackingCV . Attributes: best_estimator_ : estimator The voting estimator associated with the highest CV score. best_score_ : scalar Highest CV score attained by any voting estimator. weights_ : array-like Weights the voting estimator places on each of the estimators in its ensemble. names_ : list List of estimator names in the best estimator.","title":"fast_automl.ensemble.RFEVotingEstimatorCV"},{"location":"api/ensemble/#methods_2","text":"fit ( self, X, y, sample_weight=None ) [source] Fit the model. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self :","title":"Methods"},{"location":"api/ensemble/#fast_automlensemblerfevotingclassifiercv","text":"Selects classifiers using recursive feature elimination. Inherits from RFEVotingEstimatorCV .","title":"fast_automl.ensemble.RFEVotingClassifierCV"},{"location":"api/ensemble/#examples_1","text":"from fast_automl.ensemble import RFEVotingClassifierCV from sklearn.datasets import load_breast_cancer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score, train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC X, y = load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True) clf = RFEVotingClassifierCV([ ('rf', RandomForestClassifier()), ('knn', KNeighborsClassifier()), ('svm', SVC(probability=True)) ]).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test)))","title":"Examples"},{"location":"api/ensemble/#fast_automlensemblerfevotingregressorcv","text":"Selects regressors using recursive feature elimination. Inherits from RFEVotingEstimatorCV .","title":"fast_automl.ensemble.RFEVotingRegressorCV"},{"location":"api/ensemble/#examples_2","text":"from fast_automl.ensemble import RFEVotingRegressorCV from sklearn.datasets import load_boston from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score, train_test_split from sklearn.neighbors import KNeighborsRegressor from sklearn.svm import SVR X, y = load_boston(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = RFEVotingRegressorCV([ ('rf', RandomForestRegressor()), ('knn', KNeighborsRegressor()), ('svm', SVR()) ]).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(reg.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(reg.score(X_test, y_test)))","title":"Examples"},{"location":"api/ensemble/#fast_automlensemblestepwisevotingestimatorcv","text":"Selects estimators using stepwise addition. Inherits from BaseStackingCV . Attributes: best_estimator_ : estimator The voting estimator associated with the highest CV score. best_score_ : scalar Highest CV score attained by any voting estimator. weights_ : array-like Weights the voting estimator places on each of the estimators in its ensemble. names_ : list List of estimator names in the best estimator.","title":"fast_automl.ensemble.StepwiseVotingEstimatorCV"},{"location":"api/ensemble/#methods_3","text":"fit ( self, X, y, sample_weight=None ) [source] Fit the model. Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self :","title":"Methods"},{"location":"api/ensemble/#fast_automlensemblestepwisevotingclassifiercv","text":"Selects classifiers using stepwise addition. Inherits from StepwiseVotingEstimatorCV .","title":"fast_automl.ensemble.StepwiseVotingClassifierCV"},{"location":"api/ensemble/#examples_3","text":"from fast_automl.ensemble import StepwiseVotingClassifierCV from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score, train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC X, y = load_iris(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True) clf = StepwiseVotingClassifierCV([ ('rf', RandomForestClassifier()), ('knn', KNeighborsClassifier()), ('svm', SVC(probability=True)) ]).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(clf.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(clf.score(X_test, y_test)))","title":"Examples"},{"location":"api/ensemble/#fast_automlensemblestepwisevotingregressorcv","text":"Selects regressors using stepwise addition. Inherits from StepwiseVotingEstimatorCV .","title":"fast_automl.ensemble.StepwiseVotingRegressorCV"},{"location":"api/ensemble/#examples_4","text":"from fast_automl.ensemble import StepwiseVotingRegressorCV from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score, train_test_split from sklearn.neighbors import KNeighborsRegressor from sklearn.svm import SVR X, y = load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = StepwiseVotingRegressorCV([ ('rf', RandomForestRegressor()), ('knn', KNeighborsRegressor()), ('svm', SVR()) ]).fit(X_train, y_train) print('CV score: {:.4f}'.format(cross_val_score(reg.best_estimator_, X_train, y_train).mean())) print('Test score: {:.4f}'.format(reg.score(X_test, y_test)))","title":"Examples"},{"location":"api/linear_model/","text":"a.src-href { float: right; } p.attr { margin-top: 0.5em; margin-left: 1em; } p.func-header { background-color: gainsboro; border-radius: 0.1em; padding: 0.5em; padding-left: 1em; } table.field-table { border-radius: 0.1em } Linear models fast_automl.linear_model. ConstrainedLinearRegression class fast_automl.linear_model. ConstrainedLinearRegression ( constraint=0, copy_X=True, n_jobs=None ) [source] Linear regression where the coefficients are constrained to sum to a given value. Parameters: constraint : scalar, default=0 Sum of the regression coefficients. normalize : bool, default=False copy_Xbool, default=True : If True, X will be copied; else, it may be overwritten. n_jobs : int, default=None The number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. Attributes: coef_ : array-like of shape (n_features,) or (n_targets, n_features) Estimated coefficients for the linear regression contrained to sum to the given constraint value. Examples from fast_automl.linear_model import ConstrainedLinearRegression from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split X, y = load_boston(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = ConstrainedLinearRegression(constraint=.8).fit(X_train, y_train) print(reg.score(X_test, y_test)) print(reg.coef_.sum()) Out: 0.6877629260102918 0.8 Methods fit ( self, X, y, sample_weight=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples, n_targets) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Parameters: X : array-like, shape (n_samples, n_features) Samples. Returns: C : array, shape (n_samples, n_targets) Predicted values fast_automl.linear_model. Ridge class fast_automl.linear_model. Ridge ( alpha=1.0, prior_weight=0, normalize_coef=False, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol= 0.001, solver='auto', random_state=None ) [source] Ridge regression with the option for custom prior weights on coefficients. Parameters: prior_weight : array-like of shape (n_features) Prior weight means. See [scikit-learn's ridge regression documentation](https : //scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) for additional parameter details. Attributes: coef_ndarray of shape (n_features,) or (n_targets, n_features) : Weight vector(s). intercept_float or ndarray of shape (n_targets,) : Independent term in decision function. Set to 0.0 if fit_intercept = False . Examples from fast_automl.linear_model import Ridge from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split X, y = load_boston(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = Ridge().fit(X_train, y_train) reg.score(X_test, y_test) Methods fit ( self, X, y, sample_weight=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples, n_targets) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Parameters: X : array-like, shape (n_samples, n_features) Samples. Returns: C : array, shape (n_samples, n_targets) Predicted values","title":"Linear models"},{"location":"api/linear_model/#linear-models","text":"","title":"Linear models"},{"location":"api/linear_model/#fast_automllinear_modelconstrainedlinearregression","text":"class fast_automl.linear_model. ConstrainedLinearRegression ( constraint=0, copy_X=True, n_jobs=None ) [source] Linear regression where the coefficients are constrained to sum to a given value. Parameters: constraint : scalar, default=0 Sum of the regression coefficients. normalize : bool, default=False copy_Xbool, default=True : If True, X will be copied; else, it may be overwritten. n_jobs : int, default=None The number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. Attributes: coef_ : array-like of shape (n_features,) or (n_targets, n_features) Estimated coefficients for the linear regression contrained to sum to the given constraint value.","title":"fast_automl.linear_model.ConstrainedLinearRegression"},{"location":"api/linear_model/#examples","text":"from fast_automl.linear_model import ConstrainedLinearRegression from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split X, y = load_boston(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = ConstrainedLinearRegression(constraint=.8).fit(X_train, y_train) print(reg.score(X_test, y_test)) print(reg.coef_.sum()) Out: 0.6877629260102918 0.8","title":"Examples"},{"location":"api/linear_model/#methods","text":"fit ( self, X, y, sample_weight=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples, n_targets) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Parameters: X : array-like, shape (n_samples, n_features) Samples. Returns: C : array, shape (n_samples, n_targets) Predicted values","title":"Methods"},{"location":"api/linear_model/#fast_automllinear_modelridge","text":"class fast_automl.linear_model. Ridge ( alpha=1.0, prior_weight=0, normalize_coef=False, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol= 0.001, solver='auto', random_state=None ) [source] Ridge regression with the option for custom prior weights on coefficients. Parameters: prior_weight : array-like of shape (n_features) Prior weight means. See [scikit-learn's ridge regression documentation](https : //scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) for additional parameter details. Attributes: coef_ndarray of shape (n_features,) or (n_targets, n_features) : Weight vector(s). intercept_float or ndarray of shape (n_targets,) : Independent term in decision function. Set to 0.0 if fit_intercept = False .","title":"fast_automl.linear_model.Ridge"},{"location":"api/linear_model/#examples_1","text":"from fast_automl.linear_model import Ridge from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split X, y = load_boston(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True) reg = Ridge().fit(X_train, y_train) reg.score(X_test, y_test)","title":"Examples"},{"location":"api/linear_model/#methods_1","text":"fit ( self, X, y, sample_weight=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples, n_targets) Target values. sample_weight, array-like of shape (n_samples,), default=Noone : Individual weights for each sample. Returns: self : predict ( self, X ) [source] Parameters: X : array-like, shape (n_samples, n_features) Samples. Returns: C : array, shape (n_samples, n_targets) Predicted values","title":"Methods"},{"location":"api/metrics/","text":"a.src-href { float: right; } p.attr { margin-top: 0.5em; margin-left: 1em; } p.func-header { background-color: gainsboro; border-radius: 0.1em; padding: 0.5em; padding-left: 1em; } table.field-table { border-radius: 0.1em } Metrics fast_automl.metrics. check_scoring def fast_automl.metrics. check_scoring ( scoring, classifier ) [source] Creates a default regression or classifier scoring rule. This is R-squared for regressors and ROC AUC for classifiers. Parameters: scoring : str or callable A str (see scikit-learn's model evaluation docs) or a scorer callable with signature scorer(estimator, X, y) which returns a single value. classifier : bool Indicates that the estimator is a classifier. fast_automl.metrics. roc_auc_score def fast_automl.metrics. roc_auc_score ( y, output ) [source] Parameters: y : array-like of shape (n_samples,) target values. output : array-like of shape (n_samples, n_classes) Predicted probability of each class. Returns: score : scalar ROC AUC score, default is one-versus-rest for multi-class problems.","title":"Metrics"},{"location":"api/metrics/#metrics","text":"","title":"Metrics"},{"location":"api/metrics/#fast_automlmetricscheck_scoring","text":"def fast_automl.metrics. check_scoring ( scoring, classifier ) [source] Creates a default regression or classifier scoring rule. This is R-squared for regressors and ROC AUC for classifiers. Parameters: scoring : str or callable A str (see scikit-learn's model evaluation docs) or a scorer callable with signature scorer(estimator, X, y) which returns a single value. classifier : bool Indicates that the estimator is a classifier.","title":"fast_automl.metrics.check_scoring"},{"location":"api/metrics/#fast_automlmetricsroc_auc_score","text":"def fast_automl.metrics. roc_auc_score ( y, output ) [source] Parameters: y : array-like of shape (n_samples,) target values. output : array-like of shape (n_samples, n_classes) Predicted probability of each class. Returns: score : scalar ROC AUC score, default is one-versus-rest for multi-class problems.","title":"fast_automl.metrics.roc_auc_score"},{"location":"api/test/","text":"a.src-href { float: right; } p.attr { margin-top: 0.5em; margin-left: 1em; } p.func-header { background-color: gainsboro; border-radius: 0.1em; padding: 0.5em; padding-left: 1em; } table.field-table { border-radius: 0.1em } Model comparison tests fast_automl.test. corrected_repeated_kfold_cv_test def fast_automl.test. corrected_repeated_kfold_cv_test ( estimators, X, y, repetitions=10, cv= 10, scoring=None, n_jobs=None ) [source] Performs pairwise corrected repeated k-fold cross-validation tests. See Bouckaert and Frank . Parameters: estimators : list List of (name, estimator) tuples. X : array-like of shape (n_samples, n_features) Features. y : array-like of shape (n_samples, n_targets) Targets. repetitions : int, default=10 Number of cross-validation repetitions. cv : int, cross-validation generator, or iterable, default=10 Scikit-learn style cv parameter. scoring : str, callable, list, tuple, or dict, default=None Scikit-learn style scoring parameter. n_jobs : int, default=None Number of jobs to run in parallel. Returns: results_df : pd.DataFrame Examples from fast_automl.test import corrected_repeated_kfold_cv_test from sklearn.datasets import load_boston from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import Ridge from sklearn.svm import SVR X, y = load_boston(return_X_y=True) corrected_repeated_kfold_cv_test( [ ('rf', RandomForestRegressor()), ('ridge', Ridge()), ('svm', SVR()) ], X, y, n_jobs=-1 ) Out: Estimator1 Estimator2 PerformanceDifference Std t-stat p-value rf ridge 0.165030 0.030266 5.452600 3.652601e-07 rf svm 0.670975 0.045753 14.665154 1.460994e-26 ridge svm 0.505945 0.045031 11.235469 2.258586e-19 fast_automl.test. r_by_k_cv_test def fast_automl.test. r_by_k_cv_test ( estimators, X, y, repetitions=5, cv=2, scoring=None, n_jobs=None ) [source] Performs pariwise RxK (usually 5x2) cross-validation tests. See here . Parameters: estimators : list List of (name, estimator) tuples. X : array-like of shape (n_samples, n_features) Features. y : array-like of shape (n_samples, n_targets) Targets. repetitions : int, default=10 Number of cross-validation repetitions. cv : int, cross-validation generator, or iterable, default=10 Scikit-learn style cv parameter. scoring : str, callable, list, tuple, or dict, default=None Scikit-learn style scoring parameter. n_jobs : int, default=None Number of jobs to run in parallel. Returns: results_df : pd.DataFrame Examples from fast_automl.test import r_by_k_cv_test from sklearn.datasets import load_boston from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import Ridge from sklearn.svm import SVR X, y = load_boston(return_X_y=True) r_by_k_cv_test( [ ('rf', RandomForestRegressor()), ('ridge', Ridge()), ('svm', SVR()) ], X, y, n_jobs=-1 ) Out: Estimator1 Estimator2 PerformanceDifference Std t-stat p-value rf ridge 0.143314 0.026026 5.506631 0.002701 rf svm 0.659547 0.035824 18.410644 0.000009 ridge svm 0.516233 0.021601 23.898480 0.000002","title":"Model comparison"},{"location":"api/test/#model-comparison-tests","text":"","title":"Model comparison tests"},{"location":"api/test/#fast_automltestcorrected_repeated_kfold_cv_test","text":"def fast_automl.test. corrected_repeated_kfold_cv_test ( estimators, X, y, repetitions=10, cv= 10, scoring=None, n_jobs=None ) [source] Performs pairwise corrected repeated k-fold cross-validation tests. See Bouckaert and Frank . Parameters: estimators : list List of (name, estimator) tuples. X : array-like of shape (n_samples, n_features) Features. y : array-like of shape (n_samples, n_targets) Targets. repetitions : int, default=10 Number of cross-validation repetitions. cv : int, cross-validation generator, or iterable, default=10 Scikit-learn style cv parameter. scoring : str, callable, list, tuple, or dict, default=None Scikit-learn style scoring parameter. n_jobs : int, default=None Number of jobs to run in parallel. Returns: results_df : pd.DataFrame","title":"fast_automl.test.corrected_repeated_kfold_cv_test"},{"location":"api/test/#examples","text":"from fast_automl.test import corrected_repeated_kfold_cv_test from sklearn.datasets import load_boston from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import Ridge from sklearn.svm import SVR X, y = load_boston(return_X_y=True) corrected_repeated_kfold_cv_test( [ ('rf', RandomForestRegressor()), ('ridge', Ridge()), ('svm', SVR()) ], X, y, n_jobs=-1 ) Out: Estimator1 Estimator2 PerformanceDifference Std t-stat p-value rf ridge 0.165030 0.030266 5.452600 3.652601e-07 rf svm 0.670975 0.045753 14.665154 1.460994e-26 ridge svm 0.505945 0.045031 11.235469 2.258586e-19","title":"Examples"},{"location":"api/test/#fast_automltestr_by_k_cv_test","text":"def fast_automl.test. r_by_k_cv_test ( estimators, X, y, repetitions=5, cv=2, scoring=None, n_jobs=None ) [source] Performs pariwise RxK (usually 5x2) cross-validation tests. See here . Parameters: estimators : list List of (name, estimator) tuples. X : array-like of shape (n_samples, n_features) Features. y : array-like of shape (n_samples, n_targets) Targets. repetitions : int, default=10 Number of cross-validation repetitions. cv : int, cross-validation generator, or iterable, default=10 Scikit-learn style cv parameter. scoring : str, callable, list, tuple, or dict, default=None Scikit-learn style scoring parameter. n_jobs : int, default=None Number of jobs to run in parallel. Returns: results_df : pd.DataFrame","title":"fast_automl.test.r_by_k_cv_test"},{"location":"api/test/#examples_1","text":"from fast_automl.test import r_by_k_cv_test from sklearn.datasets import load_boston from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import Ridge from sklearn.svm import SVR X, y = load_boston(return_X_y=True) r_by_k_cv_test( [ ('rf', RandomForestRegressor()), ('ridge', Ridge()), ('svm', SVR()) ], X, y, n_jobs=-1 ) Out: Estimator1 Estimator2 PerformanceDifference Std t-stat p-value rf ridge 0.143314 0.026026 5.506631 0.002701 rf svm 0.659547 0.035824 18.410644 0.000009 ridge svm 0.516233 0.021601 23.898480 0.000002","title":"Examples"},{"location":"api/utils/","text":"a.src-href { float: right; } p.attr { margin-top: 0.5em; margin-left: 1em; } p.func-header { background-color: gainsboro; border-radius: 0.1em; padding: 0.5em; padding-left: 1em; } table.field-table { border-radius: 0.1em } Utilities fast_automl.utils. TransformerMixin Version of scikit-learn's TransformerMixin which implements a default inert fit method. Methods fit ( self, X, y=None ) [source] This function doesn't do anything, but is necessary to include the transformer in a Pipeline . Returns: self : transform ( self, X ) [source] Must be implemented by the transformer. fast_automl.utils. ColumnSelector class fast_automl.utils. ColumnSelector ( columns ) [source] Selects columns from a dataframe. Parameters: columns : list List of columns to select. Examples from fast_automl.utils import ColumnSelector import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline X = pd.DataFrame({ 'x0': [-1, -2, 1, 2], 'x1': [-1, -1, 1, 1] }) y = np.array([1, 1, 2, 2]) reg = make_pipeline( ColumnSelector(['x1']), LinearRegression() ).fit(X, y) reg.score(X, y) Methods transform ( self, X, y=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : optional, array-like of shape (n_samples, n_targets) Target values. Returns: X or (X, y) : Where X columns have been selected fast_automl.utils. ColumnRemover class fast_automl.utils. ColumnRemover ( columns ) [source] Removes columns from a dataframe. Parameters: columns : list List of columns to remove. Examples from fast_automl.utils import ColumnRemover import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline X = pd.DataFrame({ 'x0': [-1, -2, 1, 2], 'x1': [-1, -1, 1, 1] }) y = np.array([1, 1, 2, 2]) reg = make_pipeline( ColumnRemover(['x0']), LinearRegression() ).fit(X, y) reg.score(X, y) Methods transform ( self, X, y=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : optional, array-like of shape (n_samples, n_targets) Target values. Returns: X or (X, y) : Where X columns have been removed fast_automl.utils. BoundRegressor class fast_automl.utils. BoundRegressor ( estimator ) [source] Constrains the predicted target value to be within the range of targets in the training data. Parameters: estimator : scikit-learn style regressor Attributes: estimator_ : scikit-learn style regressor Fitted regressor. y_max_ : scalar Maximum target value in training data. y_min_ : scalar Minimum target value in training data. Examples from fast_automl.utils import BoundRegressor import numpy as np from sklearn.linear_model import LinearRegression X_train = np.array([ [1, 2], [7, 8] ]) X_test = np.array([ [3, 4], [5, 1000] ]) y_train = np.array([1.5, 7.5]) y_test = np.array([3.5, 5.5]) reg = LinearRegression().fit(X_train, y_train) reg.predict(X_test) Out: array([3.5, 7.5]) Methods fit ( self, X, y, sample_weight=None ) [source] predict ( self, X ) [source]","title":"Utilities"},{"location":"api/utils/#utilities","text":"","title":"Utilities"},{"location":"api/utils/#fast_automlutilstransformermixin","text":"Version of scikit-learn's TransformerMixin which implements a default inert fit method.","title":"fast_automl.utils.TransformerMixin"},{"location":"api/utils/#methods","text":"fit ( self, X, y=None ) [source] This function doesn't do anything, but is necessary to include the transformer in a Pipeline . Returns: self : transform ( self, X ) [source] Must be implemented by the transformer.","title":"Methods"},{"location":"api/utils/#fast_automlutilscolumnselector","text":"class fast_automl.utils. ColumnSelector ( columns ) [source] Selects columns from a dataframe. Parameters: columns : list List of columns to select.","title":"fast_automl.utils.ColumnSelector"},{"location":"api/utils/#examples","text":"from fast_automl.utils import ColumnSelector import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline X = pd.DataFrame({ 'x0': [-1, -2, 1, 2], 'x1': [-1, -1, 1, 1] }) y = np.array([1, 1, 2, 2]) reg = make_pipeline( ColumnSelector(['x1']), LinearRegression() ).fit(X, y) reg.score(X, y)","title":"Examples"},{"location":"api/utils/#methods_1","text":"transform ( self, X, y=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : optional, array-like of shape (n_samples, n_targets) Target values. Returns: X or (X, y) : Where X columns have been selected","title":"Methods"},{"location":"api/utils/#fast_automlutilscolumnremover","text":"class fast_automl.utils. ColumnRemover ( columns ) [source] Removes columns from a dataframe. Parameters: columns : list List of columns to remove.","title":"fast_automl.utils.ColumnRemover"},{"location":"api/utils/#examples_1","text":"from fast_automl.utils import ColumnRemover import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline X = pd.DataFrame({ 'x0': [-1, -2, 1, 2], 'x1': [-1, -1, 1, 1] }) y = np.array([1, 1, 2, 2]) reg = make_pipeline( ColumnRemover(['x0']), LinearRegression() ).fit(X, y) reg.score(X, y)","title":"Examples"},{"location":"api/utils/#methods_2","text":"transform ( self, X, y=None ) [source] Parameters: X : array-like of shape (n_samples, n_features) Training data. y : optional, array-like of shape (n_samples, n_targets) Target values. Returns: X or (X, y) : Where X columns have been removed","title":"Methods"},{"location":"api/utils/#fast_automlutilsboundregressor","text":"class fast_automl.utils. BoundRegressor ( estimator ) [source] Constrains the predicted target value to be within the range of targets in the training data. Parameters: estimator : scikit-learn style regressor Attributes: estimator_ : scikit-learn style regressor Fitted regressor. y_max_ : scalar Maximum target value in training data. y_min_ : scalar Minimum target value in training data.","title":"fast_automl.utils.BoundRegressor"},{"location":"api/utils/#examples_2","text":"from fast_automl.utils import BoundRegressor import numpy as np from sklearn.linear_model import LinearRegression X_train = np.array([ [1, 2], [7, 8] ]) X_test = np.array([ [3, 4], [5, 1000] ]) y_train = np.array([1.5, 7.5]) y_test = np.array([3.5, 5.5]) reg = LinearRegression().fit(X_train, y_train) reg.predict(X_test) Out: array([3.5, 7.5])","title":"Examples"},{"location":"api/utils/#methods_3","text":"fit ( self, X, y, sample_weight=None ) [source] predict ( self, X ) [source]","title":"Methods"}]}