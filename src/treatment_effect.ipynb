{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T23:22:32.016196Z",
     "start_time": "2021-01-05T23:22:30.449458Z"
    }
   },
   "outputs": [],
   "source": [
    "from preprocess import Preprocessor\n",
    "\n",
    "import ml_inference\n",
    "from ml_inference import (\n",
    "    BaselineRegressor, AutoRegressor, cross_val_plot, error_plot\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "INFILE = '../data/pennycook_et_al_study2_clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:39:06.308688Z",
     "start_time": "2021-01-06T14:39:06.225635Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(INFILE)\n",
    "X, treat, y = df.drop(columns='Diff'), df.Treatment, df.Diff\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T23:22:36.639243Z",
     "start_time": "2021-01-05T23:22:36.631261Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "class BaselineTreatmentRegressor(LinearRegression):\n",
    "    def __init__(self, treatment_var):\n",
    "        self.treatment_var = treatment_var\n",
    "        super().__init__()\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        return super().fit(self.transform_X(X), y, sample_weight)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return super().predict(self.transform_X(X))\n",
    "    \n",
    "    def transform_X(self, X):\n",
    "        return (\n",
    "            X[self.treatment_var].values if isinstance(X, pd.DataFrame)\n",
    "            else X[:, self.treatment_var]\n",
    "        ).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T23:22:37.411733Z",
     "start_time": "2021-01-05T23:22:37.408111Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_treatment_idx(treatment_var, X):\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "         X = pd.DataFrame(X)\n",
    "    return pd.get_dummies(X[treatment_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:57:13.070150Z",
     "start_time": "2021-01-06T14:57:13.024474Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class TreatmentRegressor():\n",
    "    def __init__(self, models, treatment_var, scoring=r2_score):\n",
    "        self.models = models\n",
    "        self.treatment_arms = list(models.keys())\n",
    "        self.treatment_var = treatment_var\n",
    "        self.scoring = scoring\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        def fit_weights(X, y):\n",
    "            X_t = self.transform_X(X)\n",
    "            self.constrained_weight_ = fit_constrained_weight(X_t, y)\n",
    "            resid = y - (X_t * self.constrained_weight_).sum(axis=1)\n",
    "            split = treatment_split(self.treatment_var, X, [X_t, resid])\n",
    "            self.weight_ = pd.DataFrame(\n",
    "                {key: fit_unconstrained_weight(X, resid) \n",
    "                for key, (_, X, resid) in split.items()}\n",
    "            )[self.treatment_arms]\n",
    "            \n",
    "        def fit_constrained_weight(X, y):\n",
    "            # constrain weights to sum to 1\n",
    "            reference = X.columns[0]\n",
    "            y -= X[reference]\n",
    "            X = X.apply(lambda x: x-X[reference]).drop(columns=reference)\n",
    "            reg = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "            return pd.Series(\n",
    "                index=self.treatment_arms,\n",
    "                data=np.insert(reg.coef_, 0, 1-reg.coef_.sum())\n",
    "            )\n",
    "        \n",
    "        def fit_unconstrained_weight(X, y):\n",
    "            # residual weights sum to 0\n",
    "            # INCREASE THE WEIGHT UNTIL ALL COEFS ARE BETWEEN 0 AND 1\n",
    "            # make this the lower bound\n",
    "            reg = Ridge(500, fit_intercept=False).fit(X, y)\n",
    "            return (reg.coef_ - reg.coef_.mean()) + self.constrained_weight_\n",
    "\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        # store means and stds for each treatment arm\n",
    "        self.mean_ = y.groupby(X[self.treatment_var]).mean()\n",
    "        self.std_ = y.groupby(X[self.treatment_var]).std()\n",
    "        # standardize y for each condition\n",
    "        treatment_idx = self.get_treatment_idx(X)\n",
    "        y = (y - treatment_idx @ self.mean_) / (treatment_idx @ self.std_)\n",
    "        # train models for each treatment arm\n",
    "        split = treatment_split(self.treatment_var, X, y)\n",
    "        [self.models[key].fit(X, y) for key, (X, y) in split.items()]\n",
    "        # fit weights\n",
    "        fit_weights(X, y)\n",
    "        return self\n",
    "    \n",
    "    def get_treatment_idx(self, X):\n",
    "        idx = get_treatment_idx(self.treatment_var, X)\n",
    "        return idx[self.treatment_arms].reset_index(drop=True)\n",
    "\n",
    "    def predict(self, X, constrained=False):\n",
    "        treatment_idx = self.get_treatment_idx(X)\n",
    "        X = self.transform_X(X)\n",
    "        if constrained:\n",
    "            y_pred = (self.constrained_weight_ * X).sum(axis=1)\n",
    "        else:\n",
    "            y_pred = ((treatment_idx @ self.weight_.T) * X).sum(axis=1)\n",
    "        return y_pred * (treatment_idx @ self.std_) + treatment_idx @ self.mean_\n",
    "    \n",
    "    def predict_effect(self, X, control_val=0, treatment_val=1):\n",
    "        def unnormalize(df, value):\n",
    "            df[value] = df[value] * self.std_[value] + self.mean_[value]\n",
    "            \n",
    "        X = self.transform_X(X)\n",
    "        y_pred = X @ self.weight_[[control_val, treatment_val]]\n",
    "        unnormalize(y_pred, control_val)\n",
    "        unnormalize(y_pred, treatment_val)\n",
    "        return y_pred[treatment_val] - y_pred[control_val]\n",
    "        \n",
    "    def transform_X(self, X):\n",
    "        X = (\n",
    "            X.drop(columns=self.treatment_var) if isinstance(X, pd.DataFrame) \n",
    "            else np.delete(X, self.treatment_var, axis=1)\n",
    "        )\n",
    "        return pd.DataFrame({\n",
    "            key: model.predict(X) for key, model in self.models.items()\n",
    "        }).reset_index()[self.treatment_arms]\n",
    "    \n",
    "    def score(self, X, y, constrained=False, *args, **kwargs):\n",
    "        return self.scoring(y, self.predict(X, constrained), *args, **kwargs)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        params = dict(\n",
    "            models=self.models,\n",
    "            treatment_var=self.treatment_var,\n",
    "            scoring=self.scoring\n",
    "        )\n",
    "#         if deep:\n",
    "#             control_params = self.control_model.get_params(deep)\n",
    "#             treat_params = self.treat_model.get_params(deep)\n",
    "#             params.update({'control__'+key: val for key, val in control_params.items()})\n",
    "#             params.update({'treat__'+key: val for key, val in treat_params.items()})\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        self.models = params['models']\n",
    "        self.treatment_var = params['treatment_var']\n",
    "        self.scorer = params['scorer']\n",
    "\n",
    "def explain_effect(reg, X, nsamples=1000, local=True, scoring=r2_score, scoring_params={}):\n",
    "    if local:\n",
    "        g = lambda effect_pred: effect_pred\n",
    "    else:\n",
    "        effect = reg.predict_effect(X)\n",
    "        g = lambda effect_pred: scoring(effect, effect_pred, **scoring_params)\n",
    "    explainer = gshap.KernelExplainer(reg.predict_effect, X, g)\n",
    "    gshap_values = explainer.gshap_values(X, nsamples=nsamples)\n",
    "    if local:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return pd.DataFrame(columns=X.columns, data=gshap_values.T)\n",
    "        return gshap_values.T\n",
    "    gshap_values /= gshap_values.sum()\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return pd.DataFrame({'Feature': X.columns, 'G-SHAP': gshap_values})\n",
    "    return gshap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T23:22:38.127632Z",
     "start_time": "2021-01-05T23:22:38.121084Z"
    }
   },
   "outputs": [],
   "source": [
    "def treatment_split(treatment_var, X, vectors=[], rm_treatment=True):\n",
    "    def split_vectors(idx):\n",
    "        if not vectors:\n",
    "            return split_vector(X, idx)\n",
    "        return [split_vector(v, idx) for v in [X]+vectors]\n",
    "    \n",
    "    def split_vector(v, idx):\n",
    "        return (\n",
    "            v[idx==1].reset_index(drop=True) if isinstance(v, (pd.DataFrame, pd.Series))\n",
    "            else v[idx]\n",
    "        )\n",
    "    \n",
    "    if not isinstance(vectors, list):\n",
    "        vectors = [vectors]\n",
    "    treatment_idx = get_treatment_idx(treatment_var, X)\n",
    "    if rm_treatment:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # treat_var is a column name\n",
    "            X = X.drop(columns=treatment_var)\n",
    "        else:\n",
    "            # treat_var is a column index\n",
    "            X = np.delete(X, treatment_var, axis=1)\n",
    "    return {key: split_vectors(idx) for key, idx in treatment_idx.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T23:52:37.574636Z",
     "start_time": "2021-01-05T23:23:01.928893Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_autoreg(treatment, X, y):\n",
    "    print('\\nTuning model for treatment arm', treatment)\n",
    "    return AutoRegressor(preprocess=Preprocessor(X), n_jobs=-1).tune(X, y, n_iter=2**5)\n",
    "\n",
    "split = treatment_split('Treatment', X, y)\n",
    "models = {key: train_autoreg(key, X, y) for key, (X, y) in split.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:57:25.255006Z",
     "start_time": "2021-01-06T14:57:18.709388Z"
    }
   },
   "outputs": [],
   "source": [
    "reg = TreatmentRegressor(models, 'Treatment')\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def make_svr_pipeline():\n",
    "    return make_pipeline(\n",
    "        Preprocessor(X),\n",
    "        SVR()\n",
    "    )\n",
    "\n",
    "models = {\n",
    "    0: make_svr_pipeline(),\n",
    "    1: make_svr_pipeline()\n",
    "}\n",
    "reg = TreatmentRegressor(models, 'Treatment')\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:58:55.807004Z",
     "start_time": "2021-01-06T14:58:54.982203Z"
    }
   },
   "outputs": [],
   "source": [
    "effect = reg.predict_effect(X)\n",
    "sns.histplot(effect, kde=True)\n",
    "effect.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T15:01:15.625011Z",
     "start_time": "2021-01-06T15:01:15.278872Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_df = reg.weight_.copy()\n",
    "weight_df = weight_df.apply(lambda x: x-reg.constrained_weight_)\n",
    "ax = sns.heatmap(weight_df, center=0, annot=True)\n",
    "ax.set(xlabel='Treatment group', ylabel='Weight on treatment-specific model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:07:07.858465Z",
     "start_time": "2021-01-06T14:02:24.345158Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "scores = []\n",
    "for i in range(10):\n",
    "    score = {'C': [], 'UC': []}\n",
    "    kf = KFold(10, shuffle=True)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        reg.fit(X_train, y_train)\n",
    "        score['UC'].append(reg.score(X_test, y_test, constrained=False))\n",
    "        score['C'].append(reg.score(X_test, y_test, constrained=True))\n",
    "    score = {key: np.array(val).mean() for key, val in score.items()}\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame(scores)\n",
    "tmp_df['Diff'] = tmp_df['UC'] - tmp_df['C']\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_1samp(tmp_df['Diff'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def moderation_test(reg, X, y, repeat=10, cv=10):\n",
    "    def compute_scores(kf, reg, X, y):\n",
    "        score = {'Unconstrained': [], 'Constrained': []}\n",
    "        # compute score for all folds\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            reg = delayed(reg.fit)(X_train, y_train)\n",
    "            score['Unconstrained'].append(\n",
    "                delayed(reg.score)(X_test, y_test, constrained=False)\n",
    "            )\n",
    "            score['Constrained'].append(\n",
    "                delayed(reg.score)(X_test, y_test, constrained=True)\n",
    "            )\n",
    "        # return mean CV score\n",
    "        return delayed(dict)(\n",
    "            Unconstrained=np.array(score['Unconstrained']).mean(), \n",
    "            Constrained=np.array(score['Constrained']).mean()\n",
    "        ).compute()\n",
    "        \n",
    "    scores = []\n",
    "    for _ in range(repeat):\n",
    "        kf = KFold(cv, shuffle=True)\n",
    "        scores.append(compute_scores(kf, reg, X, y))\n",
    "    scores = pd.DataFrame(scores)\n",
    "    scores['Delta'] = scores['Unconstrained'] - scores['Constrained']\n",
    "    res = ttest_1samp(df['Diff'], 0)\n",
    "    # convert p-value for 1-tailed hypothesis\n",
    "    pvalue = res.pvalue/2 if res.statistic > 0 else (1-res.pvalue/2)\n",
    "    return scores, res.statistic, pvalue\n",
    "\n",
    "scores, t, p = moderation_test(reg, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def moderation_test(reg, X, y, repeat=10, cv=10):\n",
    "    def compute_scores(kf, reg, X, y):\n",
    "#         reg = deepcopy(reg)\n",
    "        reg = clone(reg)\n",
    "        X, y = X.copy(), y.copy()\n",
    "        score = {'Unconstrained': [], 'Constrained': []}\n",
    "        # compute score for all folds\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            reg = delayed(reg.fit)(X_train, y_train)\n",
    "            score['Unconstrained'].append(\n",
    "                delayed(reg.score)(X_test, y_test, constrained=False)\n",
    "            )\n",
    "            score['Constrained'].append(\n",
    "                delayed(reg.score)(X_test, y_test, constrained=True)\n",
    "            )\n",
    "        # return mean CV score\n",
    "        return dict(\n",
    "            Unconstrained=np.array(score['Unconstrained']).mean(), \n",
    "            Constrained=np.array(score['Constrained']).mean()\n",
    "        )\n",
    "        \n",
    "    scores = []\n",
    "    for _ in range(repeat):\n",
    "        kf = KFold(cv, shuffle=True)\n",
    "        scores.append(compute_scores(kf, reg, X, y))\n",
    "    scores = delayed(pd.DataFrame)(scores).compute()\n",
    "    scores['Diff'] = scores['Unconstrained'] - scores['Constrained']\n",
    "    res = ttest_1samp(scores['Diff'], 0)\n",
    "    # convert p-value for 1-tailed hypothesis\n",
    "    pvalue = res.pvalue/2 if res.statistic > 0 else (1-res.pvalue/2)\n",
    "    return scores, res.statistic, pvalue\n",
    "\n",
    "scores, t, p = moderation_test(reg, X, y, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T14:31:02.813544Z",
     "start_time": "2021-01-06T14:31:02.790468Z"
    }
   },
   "outputs": [],
   "source": [
    "t, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-inference",
   "language": "python",
   "name": "ml-inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
